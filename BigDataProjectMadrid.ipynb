{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Madrid Air Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die folgenden Daten müssen von kaggle.com kopiert, entpackt und dann in Databricks geladen werden:\n",
    "- air-quality-madrid.zip                   151 MB\n",
    "- weather_madrid_LEMD_1997_2015.csv.zip    148 KB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysieren der Wetter Daten von Madrid der Jahre 1997 - 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sqlContext' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-032a844bda84>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mweather\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"com.databricks.spark.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"header\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"true\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"inferSchema\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"true\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/FileStore/tables/weather_madrid_LEMD_1997_2015.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'sqlContext' is not defined"
     ]
    }
   ],
   "source": [
    "weather = sqlContext.read.format(\"com.databricks.spark.csv\")\\\n",
    ".option(\"header\",\"true\")\\\n",
    ".option(\"inferSchema\",\"true\")\\\n",
    ".load(\"/FileStore/tables/Madrid/weather_madrid_LEMD_1997_2015.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.registerTempTable(\"weatherT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1 = sqlContext.sql(\"SELECT CET, `Max TemperatureC`, `Min TemperatureC`, `Mean TemperatureC` FROM weatherT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.clf()\n",
    "ax = plt.gca()\n",
    "pdDF = res1.toPandas()\n",
    "pdDF.plot(x='CET', y='Max TemperatureC', ax=ax)\n",
    "pdDF.plot(x='CET', y='Min TemperatureC', ax=ax)\n",
    "pdDF.plot(x='CET', y='Mean TemperatureC', ax=ax)\n",
    "display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysieren der Daten der Luftqualität von Madrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Messstationen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Einlesen der Daten der Messstationen aus CSV-File:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = sqlContext.read.format(\"com.databricks.spark.csv\")\\\n",
    ".option(\"header\",\"true\")\\\n",
    ".option(\"inferSchema\",\"true\")\\\n",
    ".load(\"/FileStore/tables/Madrid/stations.csv\")\n",
    "stations.registerTempTable(\"stationsT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stationsRes = sqlContext.sql(\"SELECT * FROM stationsT\")\n",
    "stationsRes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stationsRes.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Luftqualitäten pro Jahr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Einlesen der Daten der Luftqualitäten pro Jahr aus CSV-File:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_2001 = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"/FileStore/tables/Madrid/madrid_2001.csv\")\n",
    "air_2002 = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"/FileStore/tables/Madrid/madrid_2002.csv\")\n",
    "air_2003 = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"/FileStore/tables/Madrid/madrid_2003.csv\")\n",
    "air_2004 = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"/FileStore/tables/Madrid/madrid_2004.csv\")\n",
    "air_2005 = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"/FileStore/tables/Madrid/madrid_2005.csv\")\n",
    "air_2006 = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"/FileStore/tables/Madrid/madrid_2006.csv\")\n",
    "air_2007 = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"/FileStore/tables/Madrid/madrid_2007.csv\")\n",
    "air_2008 = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"/FileStore/tables/Madrid/madrid_2008.csv\")\n",
    "air_2009 = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"/FileStore/tables/Madrid/madrid_2009.csv\")\n",
    "air_2010 = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"/FileStore/tables/Madrid/madrid_2010.csv\")\n",
    "air_2011 = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"/FileStore/tables/Madrid/madrid_2011.csv\")\n",
    "air_2012 = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"/FileStore/tables/Madrid/madrid_2012.csv\")\n",
    "air_2013 = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"/FileStore/tables/Madrid/madrid_2013.csv\")\n",
    "air_2014 = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"/FileStore/tables/Madrid/madrid_2014.csv\")\n",
    "air_2015 = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"/FileStore/tables/Madrid/madrid_2015.csv\")\n",
    "air_2016 = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"/FileStore/tables/Madrid/madrid_2016.csv\")\n",
    "air_2017 = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"/FileStore/tables/Madrid/madrid_2017.csv\")\n",
    "air_2018 = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"/FileStore/tables/Madrid/madrid_2018.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daten von 2001 ansehen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_2001.registerTempTable(\"air_2001T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res2001 = sqlContext.sql(\"SELECT date, CO, NO_2, station FROM air_2001T\")\n",
    "res2001.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daten von 2001 der Station 28079007 darstellen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res2001S007 = sqlContext.sql(\"SELECT date, CO, NO_2, station FROM air_2001T WHERE station=28079007\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "ax = plt.gca()\n",
    "pdDF = res2001S007.toPandas()\n",
    "#pdDF.plot(x='date', y='CO', ax=ax)\n",
    "pdDF.plot(x='date', y='NO_2', ax=ax)\n",
    "display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vereinigung der Jahres-Datentabellen zu einem File.\n",
    "Dabei werden/müssen einzelne Kolonnen angepasst bzw. entfernt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_df1 = air_2001.unionByName(air_2002).unionAll(air_2003)\n",
    "air_df2 = air_2004.unionByName(air_2005).unionAll(air_2006).unionAll(air_2007).unionAll(air_2008).unionAll(air_2009).unionAll(air_2010)\n",
    "air_df3 = air_2011.unionByName(air_2012).unionAll(air_2013).unionAll(air_2014).unionAll(air_2015).unionAll(air_2016)\n",
    "air_df4 = air_2017.unionByName(air_2018)\n",
    "air_df1.printSchema()\n",
    "air_df2.printSchema()\n",
    "air_df3.printSchema()\n",
    "air_df4.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_columns = list(set(air_df1.columns).intersection(air_df2.columns).intersection(air_df3.columns).intersection(air_df4.columns))\n",
    "print(common_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in list(set(air_df1.columns) - set(common_columns)):\n",
    "  air_df1 = air_df1.drop(column)\n",
    "air_df1.printSchema()\n",
    "for column in list(set(air_df2.columns) - set(common_columns)):\n",
    "  air_df2 = air_df2.drop(column)\n",
    "air_df2.printSchema()\n",
    "for column in list(set(air_df3.columns) - set(common_columns)):\n",
    "  air_df3 = air_df3.drop(column)\n",
    "air_df3.printSchema()\n",
    "for column in list(set(air_df4.columns) - set(common_columns)):\n",
    "  air_df4 = air_df4.drop(column)\n",
    "air_df4.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_df = air_df1.unionByName(air_df2).unionByName(air_df3).unionByName(air_df4)\n",
    "air_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anzahl der Messreihen ausgeben:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#air_df = air_df.dropDuplicates()\n",
    "air_df.registerTempTable(\"airT\")\n",
    "res1 = sqlContext.sql(\"SELECT count(*) FROM airT\")\n",
    "res1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSV-File schreiben mit allen Messreihen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.fs.rm(\"/FileStore/tables/Madrid/madrid_2001-2018\", True)\n",
    "air_df.repartition(1).write.csv(\"/FileStore/tables/Madrid/madrid_2001-2018\", sep=\",\",header = 'true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parquet-File mit allen Messreihen schreiben:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_df.write.parquet(\"/FileStore/tables/Madrid/madrid_2001-2018.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anpassen des Zeitstempel-Formats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_df_pd = air_df.toPandas()\n",
    "air_df_pd[\"red_datetime\"] = [air_df_pd[\"date\"][i].replace(hour = 0, minute = 0) for i in range(len(air_df_pd))]\n",
    "air_df_pd.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlCtx = SQLContext(sc)\n",
    "air_df_enhanced = sqlCtx.createDataFrame(air_df_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anpassen der Werte durch Berechnung des Durchschnitts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_df_enhanced.registerTempTable(\"airT_enh\")\n",
    "time_gas_con = sqlContext.sql(\"SELECT red_datetime, station, AVG(BEN) AS BEN, AVG(CO) AS CO, AVG(EBE) AS EBE, AVG(NMHC) AS NMHC, AVG(NO_2) AS NO_2, AVG(O_3) AS O_3, AVG(PM10) AS PM10, AVG(SO_2) AS SO_2, AVG(TCH) AS TCH, AVG(TOL) AS TOL FROM airT_enh GROUP BY red_datetime, station\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_gas_con.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Darstellung der Anzahl Messreihen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(time_gas_con.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ausgabe der neuen Tabelle in Parquet-File:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_gas_con.write.parquet(\"/FileStore/tables/Madrid/madrid_2001-2018_time_gas_concentration.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "name": "BigDataProjectMadrid",
  "notebookId": 352219758004227
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
