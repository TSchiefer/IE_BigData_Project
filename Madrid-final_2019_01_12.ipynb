{"cells":[{"cell_type":"code","source":["# Read weather information frmo csv file for years 1997 to 2015 on a daily basis\nweather = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"/FileStore/tables/Madrid/weather_madrid_LEMD_1997_2015.csv\")\nweather.printSchema()"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["# check number of records\nweather.count()"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["# inspect 3 elements of the table\nweather.take(3)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["# register weather Dataframe to allow SQL queries and test with a query\nweather.registerTempTable(\"weatherT\")\nres1 = sqlContext.sql(\"SELECT CET, `Max TemperatureC`, `Min TemperatureC`, `Mean TemperatureC` FROM weatherT\")\nres1.show()"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["# Create temperature plot showing min, max and mean\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nspark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\") # to improve performance of Pandas conversion\n\nplt.clf()\nax = plt.gca()\npdDF = res1.toPandas()\npdDF.plot(x='CET', y='Max TemperatureC', ax=ax)\npdDF.plot(x='CET', y='Min TemperatureC', ax=ax)\npdDF.plot(x='CET', y='Mean TemperatureC', ax=ax)\ndisplay()"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["# selection to allow plot on Visibility\nres2 = sqlContext.sql(\"SELECT CET, ` Max VisibilityKm`, ` Mean VisibilityKm`, ` Min VisibilitykM` FROM weatherT\")\nres2.printSchema()"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["res2.show()"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["# create Visibility Plot\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nplt.clf()\nax = plt.gca()\npdDF = res2.toPandas()\n#pdDF.plot(x='CET', y=' Max VisibilityKm', ax=ax)\n#pdDF.plot(x='CET', y=' Min VisibilitykM', ax=ax)\npdDF.plot(x='CET', y=' Mean VisibilityKm', ax=ax)\ndisplay()"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["# read stations where air pollution is measured from CSV file\nstations = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"/FileStore/tables/Madrid/stations.csv\")\nstations.printSchema()"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["stations.count()"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["# register Dataframe for SQL queries and make test query\nstations.registerTempTable(\"stationsT\")\nres1 = sqlContext.sql(\"SELECT name, elevation FROM stationsT\")\nres1.show()"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["# read single air pollution files for years 2001 to 2018\nair_2001 = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"/FileStore/tables/Madrid/madrid_2001.csv\")\nair_2002 = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"/FileStore/tables/Madrid/madrid_2002.csv\")\nair_2003 = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"/FileStore/tables/Madrid/madrid_2003.csv\")\nair_2004 = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"/FileStore/tables/Madrid/madrid_2004.csv\")\nair_2005 = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"/FileStore/tables/Madrid/madrid_2005.csv\")\nair_2006 = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"/FileStore/tables/Madrid/madrid_2006.csv\")\nair_2007 = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"/FileStore/tables/Madrid/madrid_2007.csv\")\nair_2008 = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"/FileStore/tables/Madrid/madrid_2008.csv\")\nair_2009 = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"/FileStore/tables/Madrid/madrid_2009.csv\")\nair_2010 = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"/FileStore/tables/Madrid/madrid_2010.csv\")\nair_2011 = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"/FileStore/tables/Madrid/madrid_2011.csv\")\nair_2012 = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"/FileStore/tables/Madrid/madrid_2012.csv\")\nair_2013 = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"/FileStore/tables/Madrid/madrid_2013.csv\")\nair_2014 = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"/FileStore/tables/Madrid/madrid_2014.csv\")\nair_2015 = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"/FileStore/tables/Madrid/madrid_2015.csv\")\nair_2016 = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"/FileStore/tables/Madrid/madrid_2016.csv\")\nair_2017 = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"/FileStore/tables/Madrid/madrid_2017.csv\")\nair_2018 = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"/FileStore/tables/Madrid/madrid_2018.csv\")\n"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["# print schema for 2001. Results from command above already show that files have different number of fields\nair_2001.printSchema()"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["# register Dataframe for SQL queries and make test query\nair_2001.registerTempTable(\"air_2001T\")\nres2001 = sqlContext.sql(\"SELECT date, CO, NO_2, station FROM air_2001T\")\nres2001.show()"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["res2001.count()"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["# extract data for one station to allow plots on CO and NO_2\nres2001S007 = sqlContext.sql(\"SELECT date, CO, NO_2, station FROM air_2001T WHERE station=28079007\")\npdDF = res2001S007.toPandas()"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["# plot NO_2 for this station\nplt.clf()\nax = plt.gca()\n#pdDF.plot(x='date', y='CO', ax=ax)\npdDF.plot(x='date', y='NO_2', ax=ax)\ndisplay()"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["# combine dataframes which already have a common schema\nair_df1 = air_2001.unionByName(air_2002).unionByName(air_2003)\nair_df2 = air_2004.unionByName(air_2005).unionByName(air_2006).unionByName(air_2007).unionByName(air_2008).unionByName(air_2009).unionByName(air_2010)\nair_df3 = air_2011.unionByName(air_2012).unionByName(air_2013).unionByName(air_2014).unionByName(air_2015).unionByName(air_2016)\nair_df4 = air_2017.unionByName(air_2018)\nair_df1.printSchema()\nair_df2.printSchema()\nair_df3.printSchema()\nair_df4.printSchema()"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["# identify common attributes for all four combined dataframes\ncommon_columns = list(set(air_df1.columns).intersection(air_df2.columns).intersection(air_df3.columns).intersection(air_df4.columns))\nprint(common_columns)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["# reduce each of the four dataframes to the common attributes\nfor column in list(set(air_df1.columns) - set(common_columns)):\n  air_df1 = air_df1.drop(column)\nair_df1.printSchema()\nfor column in list(set(air_df2.columns) - set(common_columns)):\n  air_df2 = air_df2.drop(column)\nair_df2.printSchema()\nfor column in list(set(air_df3.columns) - set(common_columns)):\n  air_df3 = air_df3.drop(column)\nair_df3.printSchema()\nfor column in list(set(air_df4.columns) - set(common_columns)):\n  air_df4 = air_df4.drop(column)\nair_df4.printSchema()"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["# now combine to one dataframe \nair_df = air_df1.unionByName(air_df2).unionByName(air_df3).unionByName(air_df4)\nair_df.printSchema()"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["# check number of records in combined dataframe for air pollution\nair_df.count()"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["# helper method to allow normalization of dates by \"deleting\" hours and minutes, i.e. reducing to day information\nimport datetime\nfrom pyspark import sql\n\ndef replace_date (zeile):\n  datum = zeile[\"date\"]\n  z = zeile.asDict()\n  z['date'] = datum.replace(hour=0, minute=0)\n  return sql.types.Row(**z)\n  \n  \n#test_row = sql.Row(date=datetime.datetime(2001, 8, 1, 1, 0), BEN=None, CO=0.3700000047683716, EBE=None, NMHC=None, NO_2=58.400001525878906, O_3=34.529998779296875, PM10=105.0, SO_2=6.340000152587891, TCH=None, TOL=None, station=28079001)\n#print(test_row)\n#print(replace_date(test_row))"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["# convert Dataframe to RDD and have a look at the RDD\nairRDD = air_df.rdd\nairRDD.take(3)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["# normalize all timestamps to day only, i.e. overwrite hour and minute by 0\nairRDD_new = airRDD.map(lambda x: replace_date(x))\nairRDD_new.take(3)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["# convert back to dataframe\nair_df_new = spark.createDataFrame(airRDD_new)\nair_df_new.createOrReplaceTempView(\"airRDD_new\")\nair_df_new.cache() # wenn caching nach count(), dann keinen Effekt auf nächste Befehle\nair_df_new.count()"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["# register for SQL queries and verify number of records\nair_df_new.registerTempTable(\"airT\")\n\nres1 = sqlContext.sql(\"SELECT count(*) FROM airT\")\nres1.show() "],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["# now save combined air data as CSV file\ndbutils.fs.rm(\"/FileStore/tables/Madrid/madrid_2001-2018.csv\", True)\nair_df_new.write.csv(\"/FileStore/tables/Madrid/madrid_2001-2018.csv\", sep=\",\",header = 'true')"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["# take a look at the size of the files (approx. 4.8 MB per file with 110 files)\n%fs ls /FileStore/tables/Madrid/madrid_2001-2018.csv"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["# now store the same data as Parquet file(s)\ndbutils.fs.rm(\"/FileStore/tables/Madrid/madrid_2001-2018.parquet\", True)\nair_df_new.write.parquet(\"/FileStore/tables/Madrid/madrid_2001-2018.parquet\")\n# dbutils.fs.rm(\"/FileStore/tables/Madrid/weather.parquet\", True)\n# weather.write.parquet(\"/FileStore/tables/Madrid/weather.parquet\")   # geht nicht weil die Spaltennamen in Weather Leerzeichen enthalten"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["# again take a look at the size (approx 500 kB per file with 110 files)\n%fs ls /FileStore/tables/Madrid/madrid_2001-2018.parquet"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["# read Parquet file\n# weatherP = spark.read.parquet(\"/FileStore/tables/Madrid/weather.parquet\")\n# weatherP.createOrReplaceTempView(\"weatherP\")\nairP = spark.read.parquet(\"/FileStore/tables/Madrid/madrid_2001-2018.parquet\")\nairP.cache()\nairP.createOrReplaceTempView(\"airP\")"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["# check whether Parquet file has been loaded correctly\nres1 = sqlContext.sql(\"SELECT count(*) FROM airP\")\nres1.show()\n# sehr schnell im Vergleich zu gleicher Abfrage mit airT"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["# check performance with air from CSV\nres1 = sqlContext.sql(\"SELECT count(*) FROM weatherT, airT WHERE weatherT.CET = airT.date\")\nres1.show()"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["# and compare with performance with air from Parquet\nres1 = sqlContext.sql(\"SELECT count(*) FROM weatherT, airP WHERE weatherT.CET = airP.date\")\nres1.show()"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["# Plots über die ganze Zeit:\nfullData = sqlContext.sql(\"SELECT date, BEN, CO, EBE, NMHC, NO_2, O_3, PM10, SO_2, TCH, TOL FROM airP WHERE station=28079036\")\nfullData.cache()\nfullData.show()"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["fullData.count()"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\") # verbessert Performance by Wandlung in Pandas dataframe\nfullDF = fullData.toPandas()"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["plt.clf()\nax = plt.gca()\nfullDF.plot(x='date', y='O_3', ax=ax)\ndisplay()"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["fullData2 = sqlContext.sql(\"SELECT date, BEN, CO, EBE, NMHC, NO_2, O_3, PM10, SO_2, TCH, TOL, station FROM airP\")\nfullData2.cache()\nfullData2.show()"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["plt.clf()\nax = plt.gca()\nfullDF2 = fullData2.toPandas()"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["fullDF2.plot(x='date', y='PM10')\ndisplay()"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["air_df_pd = air_df.toPandas()\nair_df_pd[\"red_datetime\"] = [air_df_pd[\"date\"][i].replace(hour = 0, minute = 0) for i in range(len(air_df_pd))]\nair_df_pd.head(2)"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["air_df_enhanced = sqlContext.createDataFrame(air_df_pd)"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["air_df_enhanced.registerTempTable(\"airT_enh\")\ntime_gas_con = sqlContext.sql(\"SELECT red_datetime, station, AVG(BEN) AS BEN, AVG(CO) AS CO, AVG(EBE) AS EBE, AVG(NMHC) AS NMHC, AVG(NO_2) AS NO_2, AVG(O_3) AS O_3, AVG(PM10) AS PM10, AVG(SO_2) AS SO_2, AVG(TCH) AS TCH, AVG(TOL) AS TOL FROM airT_enh GROUP BY red_datetime, station\")"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"code","source":["time_gas_con.cache()\ntime_gas_con.take(10)"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"code","source":["# Darstellung der Anzahl Messreihen:\ntime_gas_con.count()"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"code","source":["# Ausgabe der neuen Tabelle in Parquet-File; erst löschen des Files, falls vorhanden:\ndbutils.fs.rm(\"/FileStore/tables/Madrid/madrid_2001-2018_time_gas_concentration.parquet\", True)\ntime_gas_con.write.parquet(\"/FileStore/tables/Madrid/madrid_2001-2018_time_gas_concentration.parquet\")"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"code","source":["air_df_grouped = air_df_new.groupBy('date').agg({'CO': 'mean', 'EBE': 'mean', 'NMHC': 'mean', 'TCH': 'mean', 'SO_2': 'mean', 'O_3': 'mean',\n                                                'TOL': 'mean', 'BEN': 'mean', 'PM10': 'mean', 'NO_2': 'mean'})\nair_df_grouped.cache()\nair_df_grouped.take(3)  # took 1.81 minutes"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"code","source":["air_df_grouped.registerTempTable(\"airGroupedT\")\nres1 = sqlContext.sql(\"SELECT count(*) FROM airGroupedT\")\nres1.show()\n# Shows nicely lacy evaluation. When run for the first time it takes around 2 minutes\n# after running and caching, takes around 3 seconds"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"code","source":["# zu Pandas\npd_air_df_grouped = air_df_grouped.toPandas()"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"code","source":["# CO-Plot:\npd_air_df_grouped.plot(x='date', y='avg(CO)')\ndisplay()"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"code","source":["# O_3 Plot\npd_air_df_grouped.plot(x='date', y='avg(O_3)')\ndisplay()"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"code","source":["# NO_2 Plot:\npd_air_df_grouped.plot(x='date', y='avg(NO_2)')\ndisplay()"],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"code","source":["# SO_2 Plot\npd_air_df_grouped.plot(x='date', y='avg(SO_2)')\ndisplay()"],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"code","source":["# PM10 Plot\npd_air_df_grouped.plot(x='date', y='avg(PM10)')\ndisplay()"],"metadata":{},"outputs":[],"execution_count":56},{"cell_type":"code","source":["# NMHC Plot\npd_air_df_grouped.plot(x='date', y='avg(NMHC)')\ndisplay()"],"metadata":{},"outputs":[],"execution_count":57},{"cell_type":"code","source":["# Join Air mit Wetterdaten\nair_weather_joined = sqlContext.sql(\"SELECT * FROM weatherT, airGroupedT WHERE weatherT.CET = airGroupedT.date\")\n# alternativ\n# air_weather_joined = air_df_grouped.join(weather, air_df_grouped.date == weather.CET, 'inner')\nair_weather_joined.take(3)"],"metadata":{},"outputs":[],"execution_count":58},{"cell_type":"code","source":["air_weather_joined.cache()\nair_weather_joined.count()"],"metadata":{},"outputs":[],"execution_count":59},{"cell_type":"code","source":["air_weather_joined.take(3)"],"metadata":{},"outputs":[],"execution_count":60},{"cell_type":"code","source":["pd_air_df_grouped = air_df_grouped.toPandas()"],"metadata":{},"outputs":[],"execution_count":61},{"cell_type":"code","source":["pd_air_df_grouped.plot(x='date', y='avg(CO)')\ndisplay()"],"metadata":{},"outputs":[],"execution_count":62},{"cell_type":"code","source":["pd_air_df_grouped.plot(x='date', y='avg(O_3)')\ndisplay()"],"metadata":{},"outputs":[],"execution_count":63},{"cell_type":"code","source":["air_weather_joined.printSchema()"],"metadata":{},"outputs":[],"execution_count":64},{"cell_type":"code","source":["import numpy as np\ndate = air_weather_joined.select(\"CET\")\ndate_pd = date.toPandas()\ntemp_air_pollution = air_weather_joined.select(\"Mean TemperatureC\", \"avg(NO_2)\", \"avg(PM10)\")\ntemp_air_pollution_pd = temp_air_pollution.toPandas()\n\nis_a_number = ~np.isnan(temp_air_pollution_pd['Mean TemperatureC']) # 2 entries of temperature are NaN and need to be thrown out. This gives indices of good values\n\n# subsetting both date_pd and temp_air_pollution_pd:\ndate_pd = date_pd[is_a_number]\ntemp_air_pollution_pd = temp_air_pollution_pd[is_a_number]"],"metadata":{},"outputs":[],"execution_count":65},{"cell_type":"code","source":["# normalized_df=(df-df.mean())/df.std()\nnormalized_temp_air_pollution_pd = (temp_air_pollution_pd - temp_air_pollution_pd.mean())/temp_air_pollution_pd.std()\nnormalized_temp_air_pollution_pd"],"metadata":{},"outputs":[],"execution_count":66},{"cell_type":"code","source":["date_temp_air_scaled_pd = pd.concat([date_pd, normalized_temp_air_pollution_pd], axis=1)\ndate_temp_air_scaled_pd[\"Mean TemperatureC\"] = date_temp_air_scaled_pd[\"Mean TemperatureC\"] - 5\ndate_temp_air_scaled_pd[\"avg(PM10)\"] = date_temp_air_scaled_pd[\"avg(PM10)\"] + 5\ndate_temp_air_scaled_pd.columns = ['Date', \"mean Temperature (norm)\", \"mean NO_2 (norm)\", \"mean PM_10 (norm)\"]\ndate_temp_air_scaled_pd"],"metadata":{},"outputs":[],"execution_count":67},{"cell_type":"code","source":["date_temp_air_scaled_pd.plot()\ndisplay()"],"metadata":{},"outputs":[],"execution_count":68},{"cell_type":"code","source":["date_temp_air_scaled_pd.corr() # Korrelationsmatrix"],"metadata":{},"outputs":[],"execution_count":69},{"cell_type":"code","source":["temp_air_pollution_extended = air_weather_joined.select(\"Mean TemperatureC\", \n                                                        \"avg(NO_2)\", \n                                                        \"avg(NMHC)\", \n                                                        \"avg(BEN)\", \n                                                        \"avg(CO)\", \n                                                        \"avg(TCH)\", \n                                                        \"avg(O_3)\", \n                                                        \"avg(EBE)\",\n                                                        \"avg(TOL)\",\n                                                        \"avg(SO_2)\",\n                                                        \"avg(PM10)\") # Vergleiche Cmd 43, aber hier: meanTemp und alle AirPoll für Korrel.-Matrix"],"metadata":{},"outputs":[],"execution_count":70},{"cell_type":"code","source":["temp_air_pollution_extended_pd = temp_air_pollution_extended.toPandas()\nkorrel_mat = temp_air_pollution_extended_pd.corr()\nkorrel_mat"],"metadata":{},"outputs":[],"execution_count":71},{"cell_type":"code","source":["def plot_corr(df,size=10): # Funktion zum Berechnen und Plotten von Korrelationsmatrix (Input ist Pandas-Dataframe)\n                           # Diese Def., weil plt.matshow(dataframe.corr()) (plt aus matplotlib) keine Beschriftung macht\n    '''Function plots a graphical correlation matrix for each pair of columns in the dataframe.\n\n    Input:\n        df: pandas DataFrame\n        size: vertical and horizontal size of the plot'''\n\n    corr = df.corr()\n    fig, ax = plt.subplots(figsize=(size, size))\n    cax = ax.matshow(corr, interpolation='nearest')\n    fig.colorbar(cax)\n    ax.matshow(corr)\n    plt.xticks(range(len(corr.columns)), corr.columns);\n    plt.yticks(range(len(corr.columns)), corr.columns);\n"],"metadata":{},"outputs":[],"execution_count":72},{"cell_type":"code","source":["plot_corr(temp_air_pollution_extended_pd, size = 15)\ndisplay()"],"metadata":{},"outputs":[],"execution_count":73}],"metadata":{"name":"Madrid","notebookId":433570727879488},"nbformat":4,"nbformat_minor":0}
